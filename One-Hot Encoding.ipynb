{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4b2e11-02c6-477b-b428-c363966b35eb",
   "metadata": {},
   "source": [
    "## Exercise : One-Hot encoding Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93740c-9fbe-4574-bf02-982ce697a78b",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f9b6964-04a7-4d2a-a07b-530724b91659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76cdf2da-1e14-4b71-9b64-7dcba5839df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Hello! My name is Safae ERAJI, an AI enthusiast. I am a student in AI and Data Science. Data Science and AI are much more fun in application, rather than just reading about it. I recommend you that you explore the world of AI and Data Science.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b7fde-3515-4ec8-a721-5ffe0855c7c7",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71fdea3e-dc1f-4ba8-9f23-48f1b9cad216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences tokenization : ['Hello!', 'My name is Safae ERAJI, an AI enthusiast.', 'I am a student in AI and Data Science.', 'Data Science and AI are much more fun in application than just thinking about it.', 'I recommend you that you explore the world of AI and Data Science.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(corpus)\n",
    "print(\"Sentences tokenization :\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eb39942-c60a-47d6-a73c-0a1ed048eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in sentence 1: ['Hello', '!']\n",
      "Words in sentence 2: ['My', 'name', 'is', 'Safae', 'ERAJI', ',', 'an', 'AI', 'enthusiast', '.']\n",
      "Words in sentence 3: ['I', 'am', 'a', 'student', 'in', 'AI', 'and', 'Data', 'Science', '.']\n",
      "Words in sentence 4: ['Data', 'Science', 'and', 'AI', 'are', 'much', 'more', 'fun', 'than', 'in', 'application', 'than', 'just', 'thinking', 'about', 'it', '.']\n",
      "Words in sentence 5: ['I', 'recommend', 'you', 'that', 'you', 'explore', 'the', 'world', 'of', 'AI', 'and', 'Data', 'Science', '.']\n"
     ]
    }
   ],
   "source": [
    "words_by_sentence = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    words = word_tokenize(sentence)\n",
    "    words_by_sentence.append(words)\n",
    "    print(f\"Words in sentence {i+1}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb365458-4456-4b0e-b69b-f85c02d9c12b",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8fc9233-0437-402f-8ab9-c4456206ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower casing in sentence 1: ['hello', '!']\n",
      "Lower casing in sentence 2: ['my', 'name', 'is', 'safae', 'eraji', ',', 'an', 'ai', 'enthusiast', '.']\n",
      "Lower casing in sentence 3: ['i', 'am', 'a', 'student', 'in', 'ai', 'and', 'data', 'science', '.']\n",
      "Lower casing in sentence 4: ['data', 'science', 'and', 'ai', 'are', 'much', 'more', 'fun', 'than', 'in', 'application', 'than', 'just', 'thinking', 'about', 'it', '.']\n",
      "Lower casing in sentence 5: ['i', 'recommend', 'you', 'that', 'you', 'explore', 'the', 'world', 'of', 'ai', 'and', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus_lower_case = []\n",
    "for i, sentence in enumerate(words_by_sentence):\n",
    "    lower_words = []\n",
    "    for word in sentence:\n",
    "        lower_words.append(word.lower())\n",
    "    print(f\"Lower casing in sentence {i+1}:\", lower_words)\n",
    "    corpus_lower_case.append(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b8e40e-6454-4944-b402-be4a6008b9ac",
   "metadata": {},
   "source": [
    "### Deleting Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea8ddfb-f1d1-48ef-8f05-9274050178b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Without stop words ==\n",
      "\n",
      "['hello', '!']\n",
      "['name', 'safae', 'eraji', ',', 'ai', 'enthusiast', '.']\n",
      "['student', 'ai', 'data', 'science', '.']\n",
      "['data', 'science', 'ai', 'much', 'fun', 'application', 'thinking', '.']\n",
      "['recommend', 'explore', 'world', 'ai', 'data', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_words = []\n",
    "print(\"== Without stop words ==\\n\")\n",
    "for sentence in corpus_lower_case:\n",
    "    words=[]\n",
    "    for word in sentence:\n",
    "        if word not in stop_words:\n",
    "            words.append(word)\n",
    "    new_words.append(words)\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7a78dce-5bfe-44ea-a404-3ca02fa294c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', '!'], ['name', 'safae', 'eraji', ',', 'ai', 'enthusiast', '.'], ['student', 'ai', 'data', 'science', '.'], ['data', 'science', 'ai', 'much', 'fun', 'application', 'thinking', '.'], ['recommend', 'explore', 'world', 'ai', 'data', 'science', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad869b6e-de33-4022-b347-53f8863744c7",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5773a403-6840-4bf6-bbba-a2bda8549fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Lemmatization ==\n",
      "['hello', '!']\n",
      "['name', 'safae', 'eraji', ',', 'ai', 'enthusiast', '.']\n",
      "['student', 'ai', 'data', 'science', '.']\n",
      "['data', 'science', 'ai', 'much', 'fun', 'application', 'thinking', '.']\n",
      "['recommend', 'explore', 'world', 'ai', 'datum', 'science', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "corpus_lemms = []\n",
    "print(\"== Lemmatization ==\")\n",
    "for token in new_words:\n",
    "    tok = ' '.join(token)\n",
    "    #print(\"tok : \", tok)\n",
    "    doc = nlp(tok)\n",
    "    #print(\"doc : \", doc)\n",
    "    corpus_lemms.append([t.lemma_ for t in doc])\n",
    "    print([t.lemma_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a25653d7-8a7f-4200-9382-f6a81f71a43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', '!'],\n",
       " ['name', 'safae', 'eraji', ',', 'ai', 'enthusiast', '.'],\n",
       " ['student', 'ai', 'data', 'science', '.'],\n",
       " ['data', 'science', 'ai', 'much', 'fun', 'application', 'thinking', '.'],\n",
       " ['recommend', 'explore', 'world', 'ai', 'datum', 'science', '.']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lemms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d43701-d2c2-4fd5-b884-141b8d683e5b",
   "metadata": {},
   "source": [
    "### Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c866079-0acf-4f3b-893b-1ff5ddcddd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire: {'explore', '!', 'much', 'thinking', 'enthusiast', 'science', 'safae', 'data', 'recommend', 'hello', '.', 'eraji', 'datum', 'fun', 'application', 'student', 'world', 'ai', ',', 'name'}\n",
      "Taille du vocabulaire: 20\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for i in corpus_lemms:\n",
    "    for j in i:\n",
    "        vocab.add(j) \n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Vocabulary Size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d0a100-ae24-4d5c-b175-fb78fb93283a",
   "metadata": {},
   "source": [
    "### Vocabulary Indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a643798-6db3-4450-9cea-75156bc5080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed Vocabulary: {'explore': 0, '!': 1, 'much': 2, 'thinking': 3, 'enthusiast': 4, 'science': 5, 'safae': 6, 'data': 7, 'recommend': 8, 'hello': 9, '.': 10, 'eraji': 11, 'datum': 12, 'fun': 13, 'application': 14, 'student': 15, 'world': 16, 'ai': 17, ',': 18, 'name': 19}\n"
     ]
    }
   ],
   "source": [
    "index_vocab = {}\n",
    "\n",
    "for i, w in enumerate(vocab):\n",
    "    index_vocab[w] = i\n",
    "\n",
    "print(\"Indexed Vocabulary:\", index_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8ea0c-d51e-47e6-a0aa-405e0d7de720",
   "metadata": {},
   "source": [
    "### One-Hot Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "289fe3a4-7ca6-40fe-aad7-4c38454dd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(word, vocab_dict=index_vocab, vocab_size=len(vocab)):\n",
    "    # A one-hot vector for a certain word\n",
    "    vector = [0] * vocab_size\n",
    "    if word in vocab_dict:\n",
    "        vector[vocab_dict[word]] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8d7fc18-137b-4041-a127-0235010aca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "vectors_one_hot=[]\n",
    "for i in corpus_lemms:\n",
    "    vector_sentences = []\n",
    "    for j in i:\n",
    "        v = create_one_hot(j)\n",
    "        vector_sentences.append(v)\n",
    "    vectors_one_hot.append(vector_sentences)\n",
    "\n",
    "# For the first sentence : \n",
    "print(vectors_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fac892bf-593e-4f32-8177-fcbb5ea233dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector of sentence 1 :  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Vector of sentence 2 :  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Vector of sentence 3 :  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Vector of sentence 4 :  [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "Vector of sentence 5 :  [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "for i, vector in enumerate(vectors_one_hot):\n",
    "    print(f\"\\nVector of sentence {i+1} : \", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84287a9f-5f41-45e5-b299-e7041e9d1502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
